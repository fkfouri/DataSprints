{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio DataSprints - Fabio Kfouri\n",
    "\n",
    "Este é um desafio dado pela <b><i>data <span style='color: red'>sprints</span></i></b> para avaliação técnica em Engenharia de Dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalando as dependencias, importando bibliotecas e configuração inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #para ignorar mensagens de warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !pip install pyspark==\"2.4.5\" --quiet\n",
    "    !pip install pandas==\"1.0.4\" --quiet\n",
    "    !pip install seaborn==\"0.9.0\" --quiet\n",
    "    !pip install matplotlib==\"3.2.2\" --quiet\n",
    "except:\n",
    "    print(\"Running throw py file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkFiles\n",
    "import pyspark\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f3692d40a26a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Desafio Data Sprints - Fabio Kfouri\"\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.msvscode.hdinsight\\hdinsightJupyter\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.msvscode.hdinsight\\hdinsightJupyter\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.msvscode.hdinsight\\hdinsightJupyter\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\.msvscode.hdinsight\\hdinsightJupyter\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.msvscode.hdinsight\\hdinsightJupyter\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.msvscode.hdinsight\\hdinsightJupyter\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[1;34m(conf, insecure)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Desafio Data Sprints - Fabio Kfouri\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação dos Dataframes\n",
    "Referenciando o endereço das fontes de dados no Bucket S3 AWS <b>s3://data-sprints-eng-test/</b>. \n",
    "\n",
    "Para desenvolvimento local foi incluido as fontes na pasta <b>/data/</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataPath = 'https://s3.amazonaws.com/data-sprints-eng-test/'\n",
    "outPath = 's3://data-sprints-fk/output/'\n",
    "\n",
    "if 'E:\\\\' in os.getcwd() and 'DataSprints' in os.getcwd():\n",
    "    #dataPath = os.getcwd() + \"/data/\"\n",
    "    outPath = os.getcwd() + \"/output/\"\n",
    "\n",
    "print(dataPath, outPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiçao dos Arquivos no SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(dataPath + 'data-payment_lookup-csv.csv')\n",
    "spark.sparkContext.addFile(dataPath + 'data-vendor_lookup-csv.csv')\n",
    "spark.sparkContext.addFile(dataPath + 'data-sample_data-nyctaxi-trips-2009-json_corrigido.json')\n",
    "spark.sparkContext.addFile(dataPath + 'data-sample_data-nyctaxi-trips-2010-json_corrigido.json')\n",
    "spark.sparkContext.addFile(dataPath + 'data-sample_data-nyctaxi-trips-2011-json_corrigido.json')\n",
    "spark.sparkContext.addFile(dataPath + 'data-sample_data-nyctaxi-trips-2012-json_corrigido.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura e Correçao da fonte Payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payment = spark.read.csv(SparkFiles.get(\"data-payment_lookup-csv.csv\"), header = True, sep = \",\")\n",
    "df_payment.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificado que a primeira linha precisa ser ignorada. Inclusao de index para auxiliar a correção. \n",
    "\n",
    "Utilização do Pandas para a leitura do CSV ignorando a linha de index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(SparkFiles.get(\"data-payment_lookup-csv.csv\"), skiprows=[0], sep=',', header=None)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Renomeando a Coluna pelp registro de Index 0;\n",
    "- Removendo o registro de Index 0;\n",
    "- Conversao do DataFrame Pandas para um DataFrama Pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.columns = temp.iloc[0]\n",
    "temp.drop(0, inplace = True)\n",
    "df_payment = spark.createDataFrame(temp)\n",
    "df_payment.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de view payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payment.createOrReplaceTempView(\"payment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura da fonte de Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendor = spark.read.csv(SparkFiles.get('data-vendor_lookup-csv.csv'), header = True, sep = \",\")\n",
    "df_vendor.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação da view vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vendor.createOrReplaceTempView(\"vendor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura das corridas de taxi no período de 2009 à 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2009 = spark.read.json(SparkFiles.get('data-sample_data-nyctaxi-trips-2009-json_corrigido.json'))\n",
    "df_2009.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2010 = spark.read.json(SparkFiles.get('data-sample_data-nyctaxi-trips-2010-json_corrigido.json'))\n",
    "df_2011 = spark.read.json(SparkFiles.get('data-sample_data-nyctaxi-trips-2011-json_corrigido.json'))\n",
    "df_2012 = spark.read.json(SparkFiles.get('data-sample_data-nyctaxi-trips-2012-json_corrigido.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do DataFrame de corridas de taxi.\n",
    "Concatenando todos os dataFrames em único DataFrame e em seguinda verificando o total de linhas do DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_2012.union(df_2011).union(df_2010).union(df_2009)\n",
    "print(\"Tamanho do DataFrame concatenado:\", df.count(), 'registros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificando o Schema do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando o aspecto dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversão de colunas [dropoff_datetime, pickup_datetime] do tipo String para tipo TimeStamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame Convertido (dfc)\n",
    "dfc = df.withColumn('dropoff_datetime', F.to_utc_timestamp('dropoff_datetime', \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "        .withColumn('pickup_datetime', F.to_utc_timestamp('pickup_datetime', \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
    "dfc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando o aspecto dos dados, em especial os campos dataTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma view trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.createOrReplaceTempView(\"trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questões do Quesito Mínimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 1: Qual a distância média percorrida por viagens com no máximo 2 passageiros.\n",
    "\n",
    "A distância média (valor arredondado) percorrida por viagens com no máximo 2 passageiros é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_1 = spark.sql(\"\"\"\n",
    "      SELECT ROUND(AVG(trip_distance),3) mean_trip_distance\n",
    "        FROM trip t\n",
    "       WHERE t.passenger_count <= 2\n",
    "\"\"\")\n",
    "df_question_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportando para um arquivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "#    os.makedirs(os.getcwd() + '/out/')\n",
    "#except:\n",
    "#    pass\n",
    "\n",
    "df_question_1.write.csv(outPath + '\\question_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 2: Quais os 3 maiores vendors em quantidade total de dinheiro arrecadado?\n",
    "\n",
    "O resultado em quantidade de dinheiro (valores em Milhões U$) arrecado pelos 3 maiores vendors são:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_2 = spark.sql(\"\"\"\n",
    "    SELECT v.name, t.vendor_id, ROUND(SUM(total_amount)/1E6,3) amount \n",
    "      FROM trip t LEFT JOIN vendor v ON (t.vendor_id = v.vendor_id)\n",
    "  GROUP BY t.vendor_id, v.name\n",
    "  ORDER BY SUM(total_amount) DESC\n",
    "     LIMIT 3\n",
    "\"\"\")\n",
    "df_question_2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportando para um arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_2.write.csv(outPath + '\\question_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 3: Um histograma da distribuição mensal, nos 4 anos, de corridas pagas em dinheiro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_3 = spark.sql(\"\"\"\n",
    "    WITH Dist as (--\n",
    "        SELECT date_format(t.dropoff_datetime,'MMM-yyyy') month_year,\n",
    "               date_format(t.dropoff_datetime,'yyyy-MM') my_idx,\n",
    "               date_format(t.dropoff_datetime,'MMM') month,\n",
    "               date_format(t.dropoff_datetime,'MM') m_idx, \n",
    "               p.payment_lookup, t.total_amount--, t.*\n",
    "          FROM trip t JOIN payment p ON (t.payment_type = p.payment_type)\n",
    "         WHERE p.payment_lookup = 'Cash' --\n",
    "    )\n",
    "    SELECT count(month) qty_trip, sum(total_amount) amount, month_year, my_idx, month, m_idx\n",
    "      FROM Dist d\n",
    "  GROUP BY month_year, my_idx, month, m_idx\n",
    "  ORDER BY my_idx\n",
    "\"\"\")\n",
    "df_question_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRansformando em Pandas\n",
    "dados = df_question_3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(dados.qty_trip)\n",
    "ax.figure.set_size_inches(20, 6)\n",
    "ax.set_xlabel('Ganho Medio', fontsize=16)\n",
    "ax.set_ylabel('Densidade', fontsize=16)\n",
    "ax.set_title(\"Distribuiçao de Media de corridas pagas em Dinheiro entre 2009 e 2012\", fontsize=20)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportando grafico\n",
    "ax.figure.savefig(outPath + '\\Question_3a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'red'\n",
    "ax1.set_title(\"Historico mensal de corridas pagas em dinheiro entre 2009 e 2012.\", fontsize=20)\n",
    "ax1.set_xlabel('Meses')\n",
    "ax1.figure.set_size_inches(20, 6)\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Eixo primario\n",
    "ax1.set_ylabel('Quantidade em milhares de corridas', color=color)\n",
    "ax1.bar(dados.month_year, dados.qty_trip/1E3, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Eixo secundario\n",
    "ax2 = ax1.twinx() \n",
    "color = 'black'\n",
    "ax2.set_ylabel('Ganhos em milhares de U$', color=color) \n",
    "ax2.plot(dados.month_year, dados.amount/1E3, color=color, linewidth=3)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportando grafico\n",
    "fig.savefig(outPath + '\\Question_3b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 4: Um gráfico de série temporal contando a quantidade de gorjetas de cada dia, nos\n",
    "últimos 3 meses de 2012:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_4 = spark.sql(\"\"\"\n",
    "WITH\n",
    "last_month AS (--\n",
    "     SELECT date_add(add_months(to_date(date_format(MAX(dropoff_datetime),'yyyy-MM') || '-01','yyyy-MM-dd'),1),-1) last_date,\n",
    "            MAX(dropoff_datetime) max_date,\n",
    "            add_months(to_date(date_format(MAX(dropoff_datetime),'yyyy-MM') || '-01','yyyy-MM-dd'),-2) first_date\n",
    "       FROM trip --\n",
    "),\n",
    "temp AS (--\n",
    "     SELECT dropoff_datetime, \n",
    "            date_format(t.dropoff_datetime,'dd-MMM-yyyy') month_year,\n",
    "            date_format(t.dropoff_datetime,'yyyy-MM-dd') my_idx, tip_amount\n",
    "       FROM trip t, last_month lm\n",
    "      WHERE dropoff_datetime between lm.first_date and lm.last_date\n",
    "        AND tip_amount > 0 -- corridas que tiveram gorjetas\n",
    "      )\n",
    "      SELECT month_year, my_idx, COUNT(tip_amount) tips from temp\n",
    "       GROUP BY month_year, my_idx\n",
    "       ORDER BY my_idx   \n",
    "\"\"\")\n",
    "df_question_4.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranformando em Pandas\n",
    "dados = df_question_4.toPandas()\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_title(\"Serie temporal da quantidade de tips nos últimos 3 meses de 2012.\", fontsize=20)\n",
    "ax1.set_xlabel('3 últimos Meses')\n",
    "ax1.figure.set_size_inches(20, 6)\n",
    "plt.xticks(rotation='vertical')\n",
    "# Eixo primario\n",
    "ax1.set_ylabel('Quantidade de Gorjetas')\n",
    "ax1.plot(dados.month_year, dados.tips ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportando grafico\n",
    "fig.savefig(outPath + '\\Question_4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questões do Quesito Bônus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 5: Qual o tempo médio das corridas nos dias de sábado e domingo?\n",
    "\n",
    "O tempo médio das corridas no fim de semana é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_5 = spark.sql(\"\"\"\n",
    "WITH calc as (--\n",
    "      SELECT dayofweek(t.dropoff_datetime) day_week_num, \n",
    "             date_format(t.dropoff_datetime, 'EEEE') day_week, \n",
    "             dropoff_datetime, pickup_datetime,\n",
    "             cast(dropoff_datetime as long) - cast(pickup_datetime as long) delta,\n",
    "             t.trip_distance\n",
    "        FROM trip t\n",
    "        WHERE dayofweek(t.dropoff_datetime) in (1,7) --\n",
    ")\n",
    "--SELECT c.*, round(delta/60, 2) delta_minutes, round(delta/60/60, 2) delta_hour FROM calc c\n",
    "    SELECT avg(delta) delta_seconds, \n",
    "           round(avg(delta/60), 2) delta_minutes, day_week\n",
    "      FROM calc\n",
    "  GROUP BY day_week\n",
    "\n",
    "\"\"\")\n",
    "df_question_5.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportando dados\n",
    "df_question_5.write.csv(outPath + '\\question_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão 6: Fazer uma visualização em mapa com latitude e longitude de pickups and dropoffs de 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_6 = spark.sql(\"\"\"\n",
    "WITH map as (--\n",
    "      SELECT dropoff_latitude latitude, dropoff_longitude longitude\n",
    "        FROM trip t\n",
    "       WHERE date_format(t.dropoff_datetime,'yyyy') = 2010\n",
    "    UNION\n",
    "     SELECT pickup_latitude, pickup_longitude\n",
    "        FROM trip t\n",
    "       WHERE date_format(t.dropoff_datetime,'yyyy') = 2010\n",
    ")\n",
    "Select *\n",
    "       --min(latitude) min_latitude, max(latitude) max_latitude, avg(latitude) avg_latitude\n",
    "       --min(longitude) min_longitude, max(longitude) max_longitude, avg(longitude) avg_longitude\n",
    "       from map m\n",
    "\"\"\")\n",
    "df_question_6.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificado a existencia de <b>Outliers</b> para Latitude e Longitude.\n",
    "\n",
    "Conversão para Pandas e realizado uma análise das Estatísticas Descritivas para identificação dos <b>Outliers</b>. \n",
    "\n",
    "Observa-se que os valores mínimo e máximo de latitude e longitude estão bem distantes dos Quartis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = df_question_6.toPandas()\n",
    "dados.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o limite inferior e superior para Latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = dados['latitude'].quantile(.25)\n",
    "Q3 = dados['latitude'].quantile(.75)\n",
    "IIQ = Q3 - Q1\n",
    "limite_inferior_latitude = Q1 - 1.5 * IIQ\n",
    "limite_superior_latitude = Q3 + 1.5 * IIQ\n",
    "print(limite_inferior_latitude, limite_superior_latitude, Q1, Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o limite inferior e superior para Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = dados['longitude'].quantile(.25)\n",
    "Q3 = dados['longitude'].quantile(.75)\n",
    "IIQ = Q3 - Q1\n",
    "limite_inferior_longitude = Q1 - 1.5 * IIQ\n",
    "limite_superior_longitude = Q3 + 1.5 * IIQ\n",
    "print(limite_inferior_longitude, limite_superior_longitude, Q1, Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a limpeza do DataSet pelos limites calculados de Latitude e Longetude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecao = (dados['latitude'] >= limite_inferior_latitude) & (dados['latitude']<= limite_superior_latitude) & \\\n",
    "          (dados['longitude'] >= limite_inferior_longitude) & (dados['longitude']<= limite_superior_longitude) \n",
    "dados_new = dados[selecao]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é um check para evidenciar que o novo DataSet é menor que o DataSet original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dados.shape, dados_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa-se que no novo Dataset não há inconsistências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_new.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo o tutorial https://towardsdatascience.com/easy-steps-to-plot-geographic-data-on-a-map-python-11217859a2db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Latitude:', dados_new['latitude'].min(), 'e', dados_new['latitude'].max())\n",
    "print('Longitude:', dados_new['longitude'].min(), 'e', dados_new['longitude'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBox = ((dados_new.longitude.min(), dados_new.longitude.max(),  \n",
    "        dados_new.latitude.min(), dados_new.latitude.max()))\n",
    "BBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foto baseada no https://www.openstreetmap.org/#map=12/40.7530/-74.0228\n",
    "\n",
    "\n",
    "![Foto Original](lib/ny_map.png) \n",
    "\n",
    "<center>Imagem de Mapa original</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_m = plt.imread(os.getcwd() + '/lib/ny_map.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_size = 60\n",
    "fig, ax = plt.subplots(figsize = (ref_size,ref_size * 1.73))\n",
    "ax.scatter(dados_new.longitude, dados_new.latitude, zorder=1, alpha= 0.2, c='b', s=10)\n",
    "ax.set_title('Traçando dados espaciais em Manhattan para corridas de 2010.', fontsize=ref_size)\n",
    "ax.set_xlim(BBox[0],BBox[1])\n",
    "ax.set_ylim(BBox[2],BBox[3])\n",
    "ax.imshow(ny_m, zorder=0, extent = BBox, aspect= 'equal')\n",
    "ax.set_xlabel('Imagem baseada no Mapa original', fontsize=ref_size *.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(outPath + '\\Question_6_map_ny_edited.png', dpi=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit533174c592bc499980d49eafc4a27dc2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
